{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa65ccf2",
   "metadata": {},
   "source": [
    "# Notebook for testing simple neural network setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8856836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np  # We need to use this numpy wrapper to make automatic differentiation work later\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Setting the random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Defining some activation functions\n",
    "def ReLU(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Compute softmax values for each set of scores in the rows of the matrix z.\n",
    "    Used with batched input data.\"\"\"\n",
    "    e_z = np.exp(z - np.max(z, axis=0))\n",
    "    return e_z / np.sum(e_z, axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "def softmax_vec(z):\n",
    "    \"\"\"Compute softmax values for each set of scores in the vector z.\n",
    "    Use this function when you use the activation function on one vector at a time\"\"\"\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / np.sum(e_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ac40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layers_batch(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    # Number of inputs in the current layer\n",
    "    i_size = network_input_size\n",
    "\n",
    "    # For each output layer size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "\n",
    "        # w has the shape of the current output layer size x the current input size\n",
    "        W = np.random.randn(i_size, layer_output_size)\n",
    "\n",
    "        # b has the shape of the current output layer size, 1\n",
    "        b = np.random.randn(1, layer_output_size)\n",
    "        \n",
    "        # Append to the layer list\n",
    "        layers.append((W, b))\n",
    "\n",
    "        # Update i_size to the output size of the current layer\n",
    "        i_size = layer_output_size\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_batch(inputs, layers, activation_funcs):\n",
    "    # Set the current a to the input vector\n",
    "    a = inputs\n",
    "\n",
    "    # For each layer and activation function\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "\n",
    "        # Calculate z for the current W and b, and the previous a\n",
    "        z = a @ W + b\n",
    "\n",
    "        # Calculate a using the given activation function\n",
    "        a = activation_func(z)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a17381",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n",
    "_ = ax.legend(\n",
    "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a083f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = iris.data\n",
    "\n",
    "# Since each prediction is a vector with a score for each of the three types of flowers,\n",
    "# we need to make each target a vector with a 1 for the correct flower and a 0 for the others.\n",
    "targets = np.zeros((len(iris.data), 3))\n",
    "for i, t in enumerate(iris.target):\n",
    "    targets[i, t] = 1\n",
    "\n",
    "\n",
    "def accuracy(predictions, targets):\n",
    "    one_hot_predictions = np.zeros(predictions.shape)\n",
    "\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        one_hot_predictions[i, np.argmax(prediction)] = 1\n",
    "    return accuracy_score(one_hot_predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predict, target):\n",
    "    return np.sum(-target * np.log(predict))\n",
    "\n",
    "\n",
    "def cost(input, layers, activation_funcs, target):\n",
    "    predict = feed_forward_batch(input, layers, activation_funcs)\n",
    "    return cross_entropy(predict, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = iris.data\n",
    "activation_funcs = [sigmoid, softmax]\n",
    "network_input_size = 4\n",
    "layer_output_sizes = [8, 3]\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048913ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "\n",
    "gradient_func = grad(cost, 1)  # Taking the gradient wrt. the second input to the cost function, i.e. the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_grad = gradient_func(inputs, layers, activation_funcs, targets)  # Don't change this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
