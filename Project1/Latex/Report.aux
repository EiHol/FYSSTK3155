\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{patrec}
\citation{Yamahata_2024}
\citation{rasch5}
\citation{johau1}
\citation{johau2}
\citation{fysml1}
\babel@aux{english}{}
\newlabel{FirstPage}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Methods}{1}{section*.2}\protected@file@percent }
\newlabel{section:methods}{{II}{1}{}{section*.2}{}}
\citation{introstat}
\citation{hastie1}
\citation{scikit-learn}
\citation{introstat}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Runge's function and polynomial interpolants $p_5(x)$ and $p_{11}(x)$.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:rungesfunction}{{1}{2}{Runge's function and polynomial interpolants $p_5(x)$ and $p_{11}(x)$}{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Maximum interpolation error $\max |f(x) - p(x)|$ for Runge's function using equally spaced data on $[-1,1]$.}}{2}{table.1}\protected@file@percent }
\newlabel{tab:runge_error}{{I}{2}{Maximum interpolation error $\max |f(x) - p(x)|$ for Runge's function using equally spaced data on $[-1,1]$}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Regression}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Ordinary Least Squares}{2}{section*.4}\protected@file@percent }
\citation{fysml2}
\citation{fysml2}
\citation{fysml2}
\citation{elestat1}
\citation{introstat1}
\citation{Raschka-et-al-2022}
\citation{fysml2}
\citation{Hastie-et-al-2009}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Ordinary Least Squares}}{3}{algorithm.1}\protected@file@percent }
\newlabel{algorithm:OLS}{{1}{3}{}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Ridge Regression}{3}{section*.5}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Ridge Regression}}{3}{algorithm.2}\protected@file@percent }
\newlabel{algorithm:Ridge}{{2}{3}{}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Lasso Regression}{3}{section*.6}\protected@file@percent }
\citation{fysml6}
\citation{introstat3}
\citation{introstat2}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Lasso Regression}}{4}{algorithm.3}\protected@file@percent }
\newlabel{algorithm:Lasso}{{3}{4}{}{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Evaluation metrics}{4}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Mean Squared Error}{4}{section*.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training and testing MSE as we vary the polynomial degree for $n$ = 50 using ordinary least squares.}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:biasvariancemse}{{2}{4}{Training and testing MSE as we vary the polynomial degree for $n$ = 50 using ordinary least squares}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Bias-variance tradeoff}{4}{section*.9}\protected@file@percent }
\newlabel{sec:bvt}{{II\,B\,2}{4}{}{section*.9}{}}
\citation{rasch3}
\citation{rasch4}
\citation{fysml7}
\citation{introstat4}
\newlabel{eq:bias_variance}{{1}{5}{}{equation.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces $Bias^2$, Variance and noise as we vary the polynomial degree for $n$ = 50 using ordinary least squares.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:biasvar}{{3}{5}{$Bias^2$, Variance and noise as we vary the polynomial degree for $n$ = 50 using ordinary least squares}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}$R^2$ Score}{5}{section*.10}\protected@file@percent }
\citation{fysml3}
\citation{fysml4}
\citation{rasch1}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Gradient decent}{6}{section*.11}\protected@file@percent }
\newlabel{sec:gd}{{II\,C}{6}{}{section*.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Gradient Descent}}{6}{algorithm.4}\protected@file@percent }
\newlabel{algorithm:GD}{{4}{6}{}{algorithm.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Contour plot of the MSE cost function with GD path from the starting point $(\theta _0, \theta _1)$ = (1,4).}}{6}{figure.4}\protected@file@percent }
\newlabel{fig:gd}{{4}{6}{Contour plot of the MSE cost function with GD path from the starting point $(\theta _0, \theta _1)$ = (1,4)}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Gradient Descent with Momentum}{6}{section*.12}\protected@file@percent }
\citation{2020NumPy-Array}
\citation{fysml5}
\citation{fysml1}
\citation{Hastie-et-al-2009}
\citation{murphy1}
\citation{fysml1}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Gradient Descent with Momentum (GDM)}}{7}{algorithm.5}\protected@file@percent }
\newlabel{algorithm:GDM}{{5}{7}{}{algorithm.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Stochastic gradient decent}{7}{section*.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Contour plot of the MSE cost function with GD and SGD path from the starting point $(\theta _0, \theta _1)$ = (1,4).}}{7}{figure.5}\protected@file@percent }
\newlabel{fig:sgd}{{5}{7}{Contour plot of the MSE cost function with GD and SGD path from the starting point $(\theta _0, \theta _1)$ = (1,4)}{figure.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Stochastic Gradient Descent (SGD)}}{7}{algorithm.6}\protected@file@percent }
\newlabel{algorithm:SGD}{{6}{7}{}{algorithm.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Resampling}{7}{section*.14}\protected@file@percent }
\citation{fysml1}
\citation{Hastie-et-al-2009}
\citation{fysml1}
\citation{scikit-learn}
\citation{introstat}
\citation{fysml1}
\citation{Raschka-et-al-2022}
\citation{murphy1}
\citation{Hastie-et-al-2009}
\citation{Hastie-et-al-2009}
\citation{Raschka-et-al-2022}
\citation{Raschka-et-al-2022}
\citation{scikit-learn}
\citation{openai2025chatgpt}
\citation{anthropic2025claude}
\citation{anthropic2025projects}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Bootstrap}{8}{section*.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Histograms of a) the original values of the dataset, b) the sampled means of 100 bootstraps, and c) the sampled means of 1000 bootstraps. The original dataset consists of 25 numbers between zero and nine. Each bootstrap samples 25 times with replacement from the original dataset per bootstrap.}}{8}{figure.6}\protected@file@percent }
\newlabel{fig:boot_ex}{{6}{8}{Histograms of a) the original values of the dataset, b) the sampled means of 100 bootstraps, and c) the sampled means of 1000 bootstraps. The original dataset consists of 25 numbers between zero and nine. Each bootstrap samples 25 times with replacement from the original dataset per bootstrap}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Cross-validation}{8}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Use of AI tools}{8}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Results and Discussion}{9}{section*.18}\protected@file@percent }
\newlabel{section:results}{{III}{9}{}{section*.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces MSE and $R^2$ for Runge's function using OLS with $\epsilon = 0$ and $\epsilon = 0.1$, $x\in [-1,1]$.}}{9}{figure.7}\protected@file@percent }
\newlabel{fig:noisecomp}{{7}{9}{MSE and $R^2$ for Runge's function using OLS with $\epsilon = 0$ and $\epsilon = 0.1$, $x\in [-1,1]$}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MSE as a function of polynomial degree and number of data points.}}{9}{figure.8}\protected@file@percent }
\newlabel{fig:OLS_n_poly}{{8}{9}{MSE as a function of polynomial degree and number of data points}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces OLS model performance for $n=50$ across polynomial degrees, showing both MSE and $R^2$ for training and test data.}}{10}{figure.9}\protected@file@percent }
\newlabel{fig:OLS_MSE}{{9}{10}{OLS model performance for $n=50$ across polynomial degrees, showing both MSE and $R^2$ for training and test data}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Magnitude of OLS coefficients as a function of polynomial degree for $n=50$. Increasing coefficient magnitude at higher degrees indicates numerical instability and overfitting.}}{10}{figure.10}\protected@file@percent }
\newlabel{fig:coef}{{10}{10}{Magnitude of OLS coefficients as a function of polynomial degree for $n=50$. Increasing coefficient magnitude at higher degrees indicates numerical instability and overfitting}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces MSE as a function of polynomial degree and number of data points, with $\lambda = 1e^{-5}$.}}{10}{figure.11}\protected@file@percent }
\newlabel{fig:ridgehm}{{11}{10}{MSE as a function of polynomial degree and number of data points, with $\lambda = 1e^{-5}$}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces }}{11}{figure.12}\protected@file@percent }
\newlabel{fig:ridgetheta}{{12}{11}{}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces MSE as a function of polynomial degree and $\lambda $, with $n=50$}}{11}{figure.13}\protected@file@percent }
\newlabel{fig:pollam}{{13}{11}{MSE as a function of polynomial degree and $\lambda $, with $n=50$}{figure.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Conclusion}{11}{section*.19}\protected@file@percent }
\newlabel{section:conclusion}{{IV}{11}{}{section*.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces caption}}{11}{figure.14}\protected@file@percent }
\newlabel{fig:}{{14}{11}{caption}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Plot MSE and $R^2$ score vs polynomial degree for Lasso regression estimate of Rugne's function, using vanilla gradient descent to optimize the estimator.}}{11}{figure.15}\protected@file@percent }
\newlabel{fig:lasso_mse_gd}{{15}{11}{Plot MSE and $R^2$ score vs polynomial degree for Lasso regression estimate of Rugne's function, using vanilla gradient descent to optimize the estimator}{figure.15}{}}
\bibdata{biblio}
\bibstyle{apsrev4-2}
\newlabel{LastPage}{{}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Plot MSE and $R^2$ score vs polynomial degree for Lasso regression estimate of Rugne's function, using gradient descent with momentum to optimize the estimator.}}{12}{figure.16}\protected@file@percent }
\newlabel{fig:lasso_mse_gdm}{{16}{12}{Plot MSE and $R^2$ score vs polynomial degree for Lasso regression estimate of Rugne's function, using gradient descent with momentum to optimize the estimator}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Comparison of test MSE between OLS, ridge and lasso regression using vanilla gradient descent trying to estimate Rugne's function. Lambda$_{Ridge} = 10^{-5}$, Lambda$_{Lasso} = 10^{-5}$}}{13}{figure.17}\protected@file@percent }
\newlabel{fig:CV_comp_GD}{{17}{13}{Comparison of test MSE between OLS, ridge and lasso regression using vanilla gradient descent trying to estimate Rugne's function. Lambda$_{Ridge} = 10^{-5}$, Lambda$_{Lasso} = 10^{-5}$}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Comparison of test MSE between OLS, ridge and lasso regression using gradient descent with momentum trying to estimate Rugne's function. Lambda$_{Ridge} = 10^{-5}$, Lambda$_{Lasso} = 10^{-5}$}}{13}{figure.18}\protected@file@percent }
\newlabel{fig:CV_comp_GDM}{{18}{13}{Comparison of test MSE between OLS, ridge and lasso regression using gradient descent with momentum trying to estimate Rugne's function. Lambda$_{Ridge} = 10^{-5}$, Lambda$_{Lasso} = 10^{-5}$}{figure.18}{}}
\gdef \@abspage@last{13}
