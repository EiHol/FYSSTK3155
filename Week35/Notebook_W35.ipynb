{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4005770",
   "metadata": {},
   "source": [
    "# Exercises week 35\n",
    "\n",
    "## Deriving and Implementing Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca1b589",
   "metadata": {},
   "source": [
    "This week you will be deriving the analytical expressions for linear regression, building up the model from scratch. This will include taking several derivatives of products of vectors and matrices. Such derivatives are central to the optimization of many machine learning models. Although we will often use automatic differentiation in actual calculations, to be able to have analytical expressions is extremely helpful in case we have simpler derivatives as well as when we analyze various properties (like second derivatives) of the chosen cost functions.\n",
    "\n",
    "Vectors are always written as boldfaced lower case letters and matrices as upper case boldfaced letters. You will find useful the notes from week 35 on derivatives of vectors and matrices. See also the textbook of Faisal at al, chapter 5 and in particular sections 5.3-5.5 at <https://github.com/CompPhysics/MachineLearning/blob/master/doc/Textbooks/MathMLbook.pdf>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e92bf9",
   "metadata": {},
   "source": [
    "### Learning goals\n",
    "\n",
    "After completing these exercises, you will know how to\n",
    "- Take the derivatives of simple products between vectors and matrices\n",
    "- Implement OLS using the analytical expressions\n",
    "- Create a feature matrix from a set of data\n",
    "- Create a feature matrix for a polynomial model\n",
    "- Evaluate the MSE score of various model on training and test data, and comparing their performance\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "Complete the following exercises while working in a jupyter notebook. Then, in canvas, include\n",
    "- The jupyter notebook with the exercises completed\n",
    "- An exported PDF of the notebook (https://code.visualstudio.com/docs/datascience/jupyter-notebooks#_export-your-jupyter-notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a9209d",
   "metadata": {},
   "source": [
    "## How to take derivatives of Matrix-Vector expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3712e",
   "metadata": {},
   "source": [
    "In these exercises it is always useful to write out with summation indices the various quantities. Take also a look at the weekly slides from week 35 and the various examples included there.\n",
    "\n",
    "As an example, consider the function\n",
    "\n",
    "$$\n",
    "f(\\boldsymbol{x}) =\\boldsymbol{A}\\boldsymbol{x},\n",
    "$$\n",
    "\n",
    "which reads for a specific component $f_i$ (we define the matrix $\\boldsymbol{A}$ to have dimension $n\\times n$ and the vector $\\boldsymbol{x}$ to have length $n$)\n",
    "\n",
    "$$\n",
    "f_i =\\sum_{j=0}^{n-1}a_{ij}x_j,\n",
    "$$\n",
    "\n",
    "which leads to\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_i}{\\partial x_j}= a_{ij},\n",
    "$$\n",
    "\n",
    "and written out in terms of the vector $\\boldsymbol{x}$ we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(\\boldsymbol{x})}{\\partial \\boldsymbol{x}}= \\boldsymbol{A}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8a4e6",
   "metadata": {},
   "source": [
    "## Exercise 1 - Finding the derivative of Matrix-Vector expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a2270",
   "metadata": {},
   "source": [
    "**a)** Consider the expression\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial \\boldsymbol{x}},\n",
    "$$\n",
    "\n",
    "Where $\\boldsymbol{a}$ and $\\boldsymbol{x}$ are column-vectors with length $n$.\n",
    "\n",
    "What is the *shape* of the expression we are taking the derivative of?\n",
    "\n",
    "What is the *shape* of the thing we are taking the derivative with respect to?\n",
    "\n",
    "What is the *shape* of the result of the expression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0396734",
   "metadata": {},
   "source": [
    "**b)** Show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial \\boldsymbol{x}} = \\boldsymbol{a}^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39d541",
   "metadata": {},
   "source": [
    "**c)** Show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{A}\\boldsymbol{a})}{\\partial \\boldsymbol{a}} = \\boldsymbol{a}^T(\\boldsymbol{A}+\\boldsymbol{A}^T),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211043d0",
   "metadata": {},
   "source": [
    "**Answers:**\\\n",
    "**1a)** \n",
    "\n",
    "Since both $\\boldsymbol{a}$ and $\\boldsymbol{x}$ are column vector with shape nx1 and we take the product of the two vectors, the product will be a 1x1 matrix.\n",
    "\n",
    "We are taking the derivative with respect to the vector $\\boldsymbol{x}$, which is an 1xn \"matrix\".\n",
    "\n",
    "Since $(\\boldsymbol{a} \\cdot \\boldsymbol{x})$ is a scalar that is differentiated with respect to $\\boldsymbol{x}$. The resulting expression will have the shape 1xn. This can be seen when looking at the Jacobian matrix and considering the result when either $(\\boldsymbol{a}$ or $\\boldsymbol{x})$ is a scalar.\n",
    "\n",
    "$$\\mathbf{J} = \\mathbf{\\frac{\\partial y}{\\partial x}} = \\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "**1b)**\\\n",
    "As previously stated the product of $(\\boldsymbol{a} \\cdot \\boldsymbol{x})$ is a scalar, a 1x1 matrix. If, instead of initialy taking the product, we keep the expression expanded. We see that $x_i$ dissapears out of the expression through the partal differentiation, leaving us with $\\boldsymbol{a}^T$.\n",
    "\n",
    "$$\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial \\boldsymbol{x}} = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial (\\boldsymbol{a_1} \\cdot \\boldsymbol{x_1})}{\\partial x_1} & \\frac{\\partial (\\boldsymbol{a_2} \\cdot \\boldsymbol{x_2})}{\\partial x_2} & \\dots & \\frac{\\partial (\\boldsymbol{a_n} \\cdot \\boldsymbol{x_n})}{\\partial x_n}\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix} \n",
    "\\boldsymbol{a_1} & \\boldsymbol{a_2} & \\dots \\boldsymbol{a_n}\n",
    "\\end{pmatrix} = \\boldsymbol{a}^T\n",
    "$$\n",
    "\n",
    "\\\n",
    "**1c)** \n",
    "\n",
    "Consider that the product of $(\\boldsymbol{a}^T\\boldsymbol{A}\\boldsymbol{a})$ is a scalar. Since A is multiplied with elements from $a$ twice, once through the left multiplication and once through the right multiplication, when differentiating $(\\boldsymbol{a}^T\\boldsymbol{A}\\boldsymbol{a})$ with respect to any element $a_i$ we get a contribution from both these elements. We can rewrite this scalar into: \n",
    "\n",
    "$$\\frac{\\partial}{\\partial a_i}\\left(\\sum_{j=1}^{n} \\sum_{k=1}^{n} a_j A_{jk} a_k\\right) = \\sum_{k=1}^{n} A_{ik} a_k + \\sum_{j=1}^{n} a_j A_{ji}$$\n",
    "\n",
    "The first sum gives us $\\boldsymbol{a}^T$ times column $i$ of $\\boldsymbol{A}^T$.\n",
    "The second sum gives us $\\boldsymbol{a}^T$ times column $i$ of $\\boldsymbol{A}$.\n",
    "\n",
    "This can be further expanded into $\\boldsymbol{a}^T(\\boldsymbol{A}^T+\\boldsymbol{A})$ through standard matrix/vector addition, showing that:\n",
    "\n",
    "$$\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{A}\\boldsymbol{a})}{\\partial \\boldsymbol{a}} = \\boldsymbol{a}^T(\\boldsymbol{A}^T + \\boldsymbol{A})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d13ec",
   "metadata": {},
   "source": [
    "## Exercise 2 - Deriving the expression for OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f771de",
   "metadata": {},
   "source": [
    "The ordinary least squares method finds the parameters $\\boldsymbol{\\theta}$ which minimizes the squared error between our model $\\boldsymbol{X\\theta}$ and the true values $\\boldsymbol{y}$.\n",
    "\n",
    "To find the parameters $\\boldsymbol{\\theta}$ which minimizes this error, we take the derivative of the squared error expression with respect to $\\boldsymbol{\\theta}$, and set it equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49690237",
   "metadata": {},
   "source": [
    "**a)** Very briefly explain why the approach above finds the parameters $\\boldsymbol{\\theta}$ which minimizes this error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cccc9d",
   "metadata": {},
   "source": [
    "We typically write the squared error as\n",
    "\n",
    "$$\n",
    "\\vert\\vert\\boldsymbol{y} - \\boldsymbol{X\\theta}\\vert\\vert^2\n",
    "$$\n",
    "\n",
    "which we can rewrite in matrix-vector form as\n",
    "\n",
    "$$\n",
    "\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbecf74",
   "metadata": {},
   "source": [
    "**b)** If $\\boldsymbol{X}$ is invertible, what is the expression for the optimal parameters $\\boldsymbol{\\theta}$? (**Hint:** Don't compute any derivatives, but solve $\\boldsymbol{X\\theta}=\\boldsymbol{y}$ for $\\boldsymbol{\\theta}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37af8f0",
   "metadata": {},
   "source": [
    "**c)** Show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)}{\\partial \\boldsymbol{s}} = -2\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\boldsymbol{A},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869fca4d",
   "metadata": {},
   "source": [
    "**d)** Using the expression from **c)**, but substituting back in $\\boldsymbol{\\theta}$, $\\boldsymbol{y}$ and $\\boldsymbol{X}$, find the expression for the optimal parameters $\\boldsymbol{\\theta}$ in the case that $\\boldsymbol{X}$ is not invertible, but $\\boldsymbol{X^T X}$ is, which is most often the case.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\theta}_{OLS}} = ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f94592",
   "metadata": {},
   "source": [
    "**Answers:**\\\n",
    "**2a)**\n",
    "\n",
    "In essence, partial derivation of an expression with respect to a given parameter gives us the gradient of that expression with respect to that parameter. Setting the gradient equal to zero lets us find the point where the given gradient is zero, or in other words; the maximum of minimum point of the expression. Given that the expression is convex, setting the gradient to zero will give us the minimum.\\\n",
    "\n",
    "\n",
    "**2b)**\n",
    "\n",
    "Given that $X$ is invertible we can multiply both sides with the inverted, $X^{-1}$. This simplifies the left side to $X^{-1} X \\theta = \\theta$, giving us $$\\theta = y X^{-1}$$\n",
    "\n",
    "**2c)**\n",
    "\n",
    "To show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)}{\\partial \\boldsymbol{s}} = -2\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\boldsymbol{A},\n",
    "$$\n",
    "\n",
    "we first set $(x-As) = k$. This gives us $\\frac{\\partial k^T k}{\\partial s}$. Empolying the chain rule we get that $\\frac{\\partial k^T k}{\\partial s} = \\frac{\\partial (k^T k)}{\\partial k} \\cdot \\frac{\\partial k}{\\partial s}$. We know that $\\frac{\\partial (k^T k)}{\\partial k} = 2k^T$, and by substituting back in $k = (x - As)$ into $\\frac{\\partial k}{\\partial s}$ and performing the partial differentiation we get $\\frac{\\partial k}{\\partial s} = -A$.\n",
    "\n",
    "Substituting both these expressions back into the initial chain rule expression, we get \n",
    "\n",
    "$$\\frac{\\partial \\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)}{\\partial \\boldsymbol{s}} = \\frac{\\partial k^T k}{\\partial s} = \\frac{\\partial (k^T k)}{\\partial k} \\cdot \\frac{\\partial k}{\\partial s} = 2k^T \\cdot -A = -2(x - As)^T A$$\n",
    "\n",
    "\n",
    "**2d**\n",
    "\n",
    "Substituting back in $\\theta, y \\text{ and } X$ into the expression from c) we get\n",
    "\n",
    "$$\\frac{\\partial (y-X\\theta)^T (y-X\\theta)}{\\partial \\theta} = -2 (y - X \\theta)^T X$$\n",
    "\n",
    "By setting the gradient equal to zero, $-2 (y - X \\theta)^T X$ = 0, we can find the optimal parameters $\\theta$. Expanding and manipulating this expression we find that $$\\hat{\\theta}_{OLS} = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca3d74",
   "metadata": {},
   "source": [
    "## Exercise 3 - Creating feature matrix and implementing OLS using the analytical expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc179f7",
   "metadata": {},
   "source": [
    "With the expression for $\\boldsymbol{\\hat{\\theta}_{OLS}}$, you now have what you need to implement OLS regression with your input data and target data $\\boldsymbol{y}$. But before you can do that, you need to set up you input data as a feature matrix $\\boldsymbol{X}$.\n",
    "\n",
    "In a feature matrix, each row is a datapoint and each column is a feature of that data. If you want to predict someones spending based on their income and number of children, for instance, you would create a row for each person in your dataset, with the montly income and the number of children as columns.\n",
    "\n",
    "We typically also include an intercept in our models. The intercept is a value that is added to our prediction regardless of the value of the other features. The intercept tries to account for constant effects in our data that are not dependant on anything else. In our current example, the intercept could account for living expenses which are typical regardless of income or childcare expenses.\n",
    "\n",
    "We calculate the optimal intercept by including a feature with the constant value of 1 in our model, which is then multplied by some parameter $\\theta_0$ from the OLS method into the optimal intercept value (which will be $\\theta_0$). In practice, we include the intercept in our model by adding a column of ones to the start of our feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ff2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3cf2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "income = np.array([116., 161., 167., 118., 172., 163., 179., 173., 162., 116., 101., 176., 178., 172., 143., 135., 160., 101., 149., 125.])\n",
    "children = np.array([5, 3, 0, 4, 5, 3, 0, 4, 4, 3, 3, 5, 1, 0, 2, 3, 2, 1, 5, 4])\n",
    "spending = np.array([152., 141., 102., 136., 161., 129.,  99., 159., 160., 107.,  98., 164., 121.,  93., 112., 127., 117.,  69., 156., 131.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da61481",
   "metadata": {},
   "source": [
    "**a)** Create a feature matrix $\\boldsymbol{X}$ for the features income and children, including an intercept column of ones at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad87a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a feature matrix based on income and number of children\n",
    "X = np.zeros((n, 3))\n",
    "X[:, 0] = 1\n",
    "X[:, 1] = income\n",
    "X[:, 2] = children**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddfac2",
   "metadata": {},
   "source": [
    "**b)** Use the expression from **3d)** to find the optimal parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ for predicting spending based on these features. Create a function for this operation, as you are going to need to use it a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3f68aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_parameters(X, y):\n",
    "\n",
    "    theta = (np.linalg.inv(X.T @ X) @ X.T ) @ y\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6da80",
   "metadata": {},
   "source": [
    "## Exercise 4 - Fitting a polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71015064",
   "metadata": {},
   "source": [
    "In this course, we typically do linear regression using polynomials, though in real world applications it is also very common to make linear models based on measured features like you did in the previous exercise.\n",
    "\n",
    "When fitting a polynomial with linear regression, we make each polynomial degree($x, x^2, x^3, ..., x^p$) its own feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7476c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.         -2.87755102 -2.75510204 -2.63265306 -2.51020408 -2.3877551\n",
      " -2.26530612 -2.14285714 -2.02040816 -1.89795918 -1.7755102  -1.65306122\n",
      " -1.53061224 -1.40816327 -1.28571429 -1.16326531 -1.04081633 -0.91836735\n",
      " -0.79591837 -0.67346939 -0.55102041 -0.42857143 -0.30612245 -0.18367347\n",
      " -0.06122449  0.06122449  0.18367347  0.30612245  0.42857143  0.55102041\n",
      "  0.67346939  0.79591837  0.91836735  1.04081633  1.16326531  1.28571429\n",
      "  1.40816327  1.53061224  1.65306122  1.7755102   1.89795918  2.02040816\n",
      "  2.14285714  2.26530612  2.3877551   2.51020408  2.63265306  2.75510204\n",
      "  2.87755102  3.        ]\n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321451b",
   "metadata": {},
   "source": [
    "**a)** Create a feature matrix $\\boldsymbol{X}$ for the features $x, x^2, x^3, x^4, x^5$, including an intercept column of ones at the start. Make this into a function, as you will do this a lot over the next weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91496e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, p):\n",
    "    n = len(x)\n",
    "    X = np.zeros((n, p + 1))\n",
    "    \n",
    "    for k in range(0, p + 1):\n",
    "        X[:, k] = x**k\n",
    "\n",
    "    return X\n",
    "\n",
    "X = polynomial_features(x, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b1e31",
   "metadata": {},
   "source": [
    "**b)** Use the expression from **3d)** to find the optimal parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ for predicting $\\boldsymbol{y}$ based on these features. If you have done everything right so far, this code will not need changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034f502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = OLS_parameters(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d703f788",
   "metadata": {},
   "source": [
    "**c)** Like in exercise 4 last week, split your feature matrix and target data into a training split and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29171358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3509f",
   "metadata": {},
   "source": [
    "**d)** Train your model on the training data(find the parameters which best fit) and compute the MSE on both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a9c93a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE function from lecture notes:\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e346f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean squared error 0.004991974924240892\n",
      "Training mean squared error 0.022852629330419674\n"
     ]
    }
   ],
   "source": [
    "# Calculating beta (theta) for the training data\n",
    "beta = OLS_parameters(X_train, y_train)\n",
    "\n",
    "# Finding ytilde based on the training features and beta\n",
    "ytilde = X_train @ beta\n",
    "\n",
    "# Calculating training MSE\n",
    "MES_train = MSE(y_train, ytilde)\n",
    "print(\"Training mean squared error\", MES_train)\n",
    "\n",
    "# Calculating test MSE\n",
    "ypredict = X_test @ beta\n",
    "MES_test = MSE(y_test, ypredict)\n",
    "print(\"Training mean squared error\", MES_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e431889",
   "metadata": {},
   "source": [
    "**e)** Do the same for each polynomial degree from 2 to 10, and plot the MSE on both the training and test data as a function of polynomial degree. The aim is to reproduce Figure 2.11 of [Hastie et al](https://github.com/CompPhysics/MLErasmus/blob/master/doc/Textbooks/elementsstat.pdf). Feel free to read the discussions leading to figure 2.11 of Hastie et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceb57457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 1: Train MSE = 0.071637, Test MSE = 0.069499\n",
      "Degree 2: Train MSE = 0.051825, Test MSE = 0.051589\n",
      "Degree 3: Train MSE = 0.022100, Test MSE = 0.021930\n",
      "Degree 4: Train MSE = 0.022100, Test MSE = 0.021888\n",
      "Degree 5: Train MSE = 0.018108, Test MSE = 0.021695\n",
      "Degree 6: Train MSE = 0.013675, Test MSE = 0.017103\n",
      "Degree 7: Train MSE = 0.011760, Test MSE = 0.015393\n",
      "Degree 8: Train MSE = 0.007839, Test MSE = 0.010820\n",
      "Degree 9: Train MSE = 0.007769, Test MSE = 0.010377\n",
      "Degree 10: Train MSE = 0.005282, Test MSE = 0.014364\n",
      "Degree 11: Train MSE = 0.005279, Test MSE = 0.014307\n",
      "Degree 12: Train MSE = 0.005259, Test MSE = 0.014406\n",
      "Degree 13: Train MSE = 0.005201, Test MSE = 0.015109\n",
      "Degree 14: Train MSE = 0.005181, Test MSE = 0.015361\n",
      "Degree 15: Train MSE = 0.004992, Test MSE = 0.022853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f021ab0380>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting seed to get the same data every time\n",
    "np.random.seed(1016)\n",
    "\n",
    "# Generate the data\n",
    "n = 50\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1, n)\n",
    "\n",
    "degrees = range(1, 16)\n",
    "MSE_train = []\n",
    "MSE_test = []\n",
    "\n",
    "# Split the data before creating the design matrices to make sure all iterations use the same test and training dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "# Iterating over all degrees\n",
    "for degree in degrees:   \n",
    "    \n",
    "    # Create a design matrix for current polynomial degree\n",
    "    X_train = polynomial_features(x_train, degree)\n",
    "    X_test = polynomial_features(x_test, degree)\n",
    "    \n",
    "    # Train the model (find optimal parameters)\n",
    "    beta = OLS_parameters(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = X_train @ beta\n",
    "    y_test_pred = X_test @ beta\n",
    "    \n",
    "    # Calculate MSE for both training and test data\n",
    "    train_mse_current = MSE(y_train, y_train_pred)\n",
    "    test_mse_current = MSE(y_test, y_test_pred)\n",
    "    \n",
    "    # Store results\n",
    "    MSE_train.append(train_mse_current)\n",
    "    MSE_test.append(test_mse_current)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Degree {degree}: Train MSE = {train_mse_current:.6f}, Test MSE = {test_mse_current:.6f}\")\n",
    "\n",
    "plt.plot(degrees, MSE_train, 'o-', color='blue', label='Training Error')\n",
    "plt.plot(degrees, MSE_test, 'o-', color='red', label='Test Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b5954",
   "metadata": {},
   "source": [
    "**f)** Interpret the graph. Why do the lines move as they do? What does it tell us about model performance and generalizability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2acfb9",
   "metadata": {},
   "source": [
    "**Answer:**\\\n",
    "**4f)**\n",
    "\n",
    "While the graph varies between runs because of the semi-random nature of data generation and test/train splitting, the general takeaway is that the MSE of the test data, as compared to the MSE of the training data, first decreses, then increases as a function of polynomial degree. This tells us that a more complex model won't always give us a better prediction. More general, we can say that there exists a point, a \"sweet spot\", where we will get the optimal performance from a model depending on the input data and model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994f0c5",
   "metadata": {},
   "source": [
    "## Exercise 5 - Comparing your code with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f595b7a",
   "metadata": {},
   "source": [
    "When implementing different algorithms for the first time, it can be helpful to double check your results with established implementations before you go on to add more complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab310c1",
   "metadata": {},
   "source": [
    "**a)** Make sure your `polynomial_features` function creates the same feature matrix as sklearns PolynomialFeatures.\n",
    "\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85b964d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Reenerate the data\n",
    "n = 50\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1, n)\n",
    "\n",
    "# Creating polynomial features using the Sklean function\n",
    "sklearn_poly = PolynomialFeatures(2).fit_transform(x.reshape(-1, 1))\n",
    "\n",
    "# Creating polynomial features using our custom function\n",
    "custom_poly = polynomial_features(x, 2)\n",
    "\n",
    "# Checking that they are the same by subtracting each element with its counterpart. If all elements are zero, the functions give the same result \n",
    "print(custom_poly - sklearn_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c32c52",
   "metadata": {},
   "source": [
    "**b)** Make sure your `OLS_parameters` function computes the same parameters as sklearns LinearRegression with fit_intercept set to False, since the intercept is included in the feature matrix. Use `your_model_object.coef_` to extract the computed parameters.\n",
    "\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35b04126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83405415  0.24070777 -0.05070171]\n",
      "[ 0.83405415  0.24070777 -0.05070171]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "custom_theta = OLS_parameters(custom_poly, y)\n",
    "print(custom_theta)\n",
    "\n",
    "sklearn_theta = LinearRegression(fit_intercept=False).fit(sklearn_poly, y).coef_\n",
    "print(sklearn_theta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
